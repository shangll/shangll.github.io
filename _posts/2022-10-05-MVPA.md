---
title: 'Decoding/MVPA - ML'
date: 2022-10-05
permalink: /posts/2025/06/MVPA/
tags:
  - python
  - Decoding
  - MVPA
  - ML
---

*注：本文基于Bae 2018年J. Neurosci上的这篇Dissociable Decoding of Spatial Attention and Working Memory from EEG Oscillations and Sustained Potentials，强烈建议阅读原文。*
*本文是仅针对懂EEG和fMRI实验并有处理经验、但无decoding经验的读者撰写的说明书；如无相关经验，不建议阅读，也不建议联系本文作者（谢谢！！！）*

#结尾有彩蛋

**Decoding在做什么、怎么做？**

人们在看不同类别图片的时候，大脑是有不同的反应模式的。如果大脑encode了这种刺激，两个种类的刺激的活动应该是不一样的。如果有一个很好的classifier，那么这两种不一样的活动就会被分类出来（所以这里我们可以看出来，decoding本质上就是在做representations）。比如我1个session 8个run，就可以training前7个run去test最后1个run。Decoding似乎fMRI用的比较早（不确定，印象中是这样），那时候还叫MVPA。这俩本质上是一个东西。那个V就是voxel的意思，到EEG中就变成了time course。如果EEG不叫decoding，那就得叫MTPA。仅此而已。

***Interim Summary***：decoding的核心原理就是我有一堆data。这堆data是二元（或者多元也行）可分的。我可以从中分出一大部分，用一个很好的classifier去training data怎么去分类，然后用trained data去test剩下的data。就是这么简单。

**下一个问题就是怎么做decoding？**

首先我们loading data：

```
import mne
epochs = mne.read_epochs('preprocessed_epochs_1.fif')
t_list = epochs.times
subj_data = epochs.get_data()
# data structure: 640 trials * 27 channels * 750 times
```

Bae这一篇是每个trial有750个time points，每个被试做了640个trials，decode 27个channels。Decoding可以ERP based也可以band power based。Bae两个都做了，我这里只展示ERP based，因为步骤都一样。我们先对已经loaded的subject 1做decoding。这里插播一个步骤***crop data***，也就是选取特定的time window。这一步不是强制的，但非常推荐——尤其是当你差marginally显著的时候。为啥？因为我们是针对time point解码。现在，我们手上的数据是250 Hz的，也就是1个时间点对应0.004"，750个time points就是3"。Bae文章中写了是onset前后1.5"。假设我们只关注1"前后，我们可以选0到1.5这个范围。相当于把baseline给剔除了。我们不做crop的话，就是把这3"全喂进去decode，效果肯定不如只做0-1.5这一段的。但话说回来，如果你差很多，这个操作没什么意义。数据这玩意，***该显著怎么做都会显著，不显著的怎么做都不显著***。

```
import munpy as np
# crop data
t0,t1 = 0.0,1.5
t0_indx,t1_indx = np.where(t_list>=t0)[0][0],np.where(t_list<=t1)[0][-1]
```
这样就剩375个time points了。我们就针对这375个time points做decoding。

接下来就是Bae decoding方案的第一步：***down sample***。什么是down sample？就是缩减time points。Bae是每5个time points平均一下。这里平均步长没有限制，你想更平滑就多平均几个。有文献算过，无论怎么average，最后结果该显著的还会显著，不会影响最终结果。如果你实在不放心，可以0-5平均一下，1-6平均一下，2-7平均一下……以此类推给自己一个心理安慰。毕竟这样你的time resolution没有降低，但结果平滑了。

为什么要resampling？其实这一步不做也不会对结果有什么实质性的影响。就想上面讲到的，down sample最主要的目的就是让结果看起来很平滑，也可以节能，属实降本增效了（除非你为了心理安慰只做平滑，那样计算时间会暴增）。如果不resample的话，data画出来会很毛躁，一堆锯齿看着不好看而已。有兴趣可以自己画画。我有些decoding没有这一步，data看着就很锯齿。还是那句话，***该显著怎么做都会显著，不显著的怎么做都不显著***。

我这里写了个sub function，也建议大家写code时候这么做，一个是看着清晰，另一方面方便重复利用——尤其是我现在是针对单个被试进行decoding。最后做统计检验肯定是group level，不可能单独被试自己去统计。

```
# down sample
t_space = 5
def resamp_t(data):
    data_avg_t = np.zeros([data.shape[0],data.shape[1],data.shape[2]])
    for t in range(t_points):
        data_avg_t[:,:,t] = np.average(data[:,:,t*t_space:t*t_space+t_space],axis=2)
    return data_avg_t

subj_data_resamp = resamp_t(subj_data)
```











